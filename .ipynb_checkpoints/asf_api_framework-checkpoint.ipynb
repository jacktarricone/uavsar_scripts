{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from datetime import datetime\n",
    "import netrc\n",
    "import getpass\n",
    "from subprocess import PIPE, Popen\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import codecs\n",
    "from osgeo import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-capitol",
   "metadata": {},
   "source": [
    "### Credentials setup for EarthData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imposed-flash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Username:  pbillecocq\n",
      "Enter Password:  ···············\n"
     ]
    }
   ],
   "source": [
    "ASF_USER = input(\"Enter Username: \")\n",
    "ASF_PASS = getpass.getpass(\"Enter Password: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-linux",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "funky-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bbox_string(polygon):\n",
    "    '''\n",
    "    Builds the string to include in the ASF search request. The bbox consists of 4 comma-separated numbers: lower left longitude,latitude, and upper right longitude,latitude.\n",
    "    \n",
    "    Parmeters\n",
    "    ----------\n",
    "    polygon: shapely polygon\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    polygon_string : string\n",
    "        String to include in the ASF request\n",
    "    '''\n",
    "    points = mapping(polygon)['coordinates'][0]\n",
    "    lower_left = points[0]\n",
    "    upper_right = points[2]\n",
    "    bbox_string = f'{lower_left[0]},{lower_left[1]},{upper_right[0]},{upper_right[1]}'\n",
    "        \n",
    "    return bbox_string\n",
    "\n",
    "def search_asf(platform, processingLevel, start, end, polygon, output_format):\n",
    "    '''\n",
    "    Search the ASF platform for images given the input parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        platform : string\n",
    "            Name of the imaging platform. Defaults to UAVSAR, but a list of supported platform is available on the ASF website\n",
    "        processingLevel : string\n",
    "            Processing level of the imaging product. \n",
    "            Possible values for UAVSAR : (KMZ, PROJECTED, PAULI, PROJECTED_ML5X5, STOKES, AMPLITUDE, BROWSE, COMPLEX, DEM_TIFF, PROJECTED_ML3X3, METADATA, AMPLITUDE_GRD, INTERFEROMETRY, INTERFEROMETRY_GRD, THUMBNAIL)\n",
    "        start : datetime object\n",
    "            Start date of the search period.\n",
    "        end : datetime object\n",
    "            End date of the search period.\n",
    "        polygon : shapely polygon defining the Area of Interest,\n",
    "        output_format: string\n",
    "            Format being returned by the ASF API. Values : CSV, JSON, KML, METALINK, COUNT, DOWNLOAD, GEOJSON\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Ouputs a search file\n",
    "    '''\n",
    "    base = 'https://api.daac.asf.alaska.edu/services/search/param'\n",
    "    start_date = start.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "    end_date = end.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "    aoi_string = build_bbox_string(polygon)\n",
    "    payload = {\n",
    "        'platform': platform,\n",
    "        'processingLevel': processingLevel,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'bbox': aoi_string,\n",
    "        'output': output_format\n",
    "    }\n",
    "    r = requests.get(base, params=payload)\n",
    "    \n",
    "    return r.json()\n",
    "\n",
    "def download_single_product(productUrl, path):\n",
    "    '''\n",
    "    Downloading function.\n",
    "    \n",
    "    Paramters\n",
    "    ----------\n",
    "    productUrl: string\n",
    "            Product download Url\n",
    "    path: string\n",
    "            path to download product to\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    Download the file\n",
    "    '''\n",
    "    process = Popen(['wget', productUrl, f'--user={ASF_USER}', f'--password={ASF_PASS}', '-P', path, '--progress=bar'], stderr=PIPE)\n",
    "    started = False\n",
    "    for line in process.stderr:\n",
    "        line = line.decode(\"utf-8\", \"replace\")\n",
    "        if started:\n",
    "            splited = line.split()\n",
    "            if len(splited) == 9:\n",
    "                percentage = splited[6]\n",
    "                speed = splited[7]\n",
    "                remaining = splited[8]\n",
    "                print(\"Downloaded {} with {} per second and {} left.\".format(percentage, speed, remaining), end='\\r')\n",
    "        elif line == os.linesep:\n",
    "            started = True\n",
    "    print(f\"\\nDone downloading {productUrl}\")\n",
    "    \n",
    "def search_and_bulk_download(platform, processingLevel, start, end, polygon, path):\n",
    "    '''\n",
    "    Search the ASF platform for images given the input parameters. Download found products\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        platform : string\n",
    "            Name of the imaging platform. Defaults to UAVSAR, but a list of supported platform is available on the ASF website\n",
    "        processingLevel : string\n",
    "            Processing level of the imaging product. \n",
    "            Possible values for UAVSAR : (KMZ, PROJECTED, PAULI, PROJECTED_ML5X5, STOKES, AMPLITUDE, BROWSE, COMPLEX, DEM_TIFF, PROJECTED_ML3X3, METADATA, AMPLITUDE_GRD, INTERFEROMETRY, INTERFEROMETRY_GRD, THUMBNAIL)\n",
    "        start : datetime object\n",
    "            Start date of the search period.\n",
    "        end : datetime object\n",
    "            End date of the search period.\n",
    "        polygon : shapely polygon defining the Area of Interest,\n",
    "        path: string\n",
    "            Path to download product to\n",
    "        '''\n",
    "    results = search_asf(platform, processingLevel, start, end, polygon, output_format='JSON')[0]\n",
    "    print(f'{len(results)} product(s) found')\n",
    "    for result in results:\n",
    "        downloadUrl = result['downloadUrl']\n",
    "        download_single_product(downloadUrl, path)\n",
    "        print('Unzipping product')\n",
    "        with zipfile.ZipFile(path + result['fileName'], 'r') as zip_ref:\n",
    "            zip_ref.extractall(path + result['productName'])\n",
    "        print('Deleting original .zip')\n",
    "        os.remove(path + result['fileName'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-prairie",
   "metadata": {},
   "source": [
    "#### Conversion to Cloud Optimized .tiff & upload to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "limited-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder is path to a folder with an .ann (or .txt) and .grd files (.amp1, .amp2, .cor, .unw, .int)\n",
    "\n",
    "def uavsar_tiff_convert(folder, verbose = False):\n",
    "    \"\"\"\n",
    "    Builds a header file for the input UAVSAR .grd file,\n",
    "    allowing the data to be read as a raster dataset.\n",
    "    :param folder:   the folder containing the UAVSAR .grd and .ann files\n",
    "    \"\"\"\n",
    "\n",
    "    os.chdir(folder)\n",
    "\n",
    "    # Empty lists to put information that will be recalled later.\n",
    "    Lines_list = []\n",
    "    Samples_list = []\n",
    "    Latitude_list = []\n",
    "    Longitude_list = []\n",
    "    Files_list = []\n",
    "\n",
    "    # Step 1: Look through folder and determine how many different flights there are\n",
    "    # by looking at the HDR files.\n",
    "    for files in os.listdir(folder):\n",
    "        if files [-4:] == \".grd\":\n",
    "            newfile = open(files[0:-4] + \".hdr\", 'w')\n",
    "            newfile.write(\"\"\"ENVI\n",
    "description = {DESCFIELD}\n",
    "samples = NSAMP\n",
    "lines = NLINE\n",
    "bands = 1\n",
    "header offset = 0\n",
    "data type = DATTYPE\n",
    "interleave = bsq\n",
    "sensor type = UAVSAR L-Band\n",
    "byte order = 0\n",
    "map info = {Geographic Lat/Lon, \n",
    "            1.000, \n",
    "            1.000, \n",
    "            LON, \n",
    "            LAT,  \n",
    "            0.0000555600000000, \n",
    "            0.0000555600000000, \n",
    "            WGS-84, units=Degrees}\n",
    "wavelength units = Unknown\n",
    "                \"\"\"\n",
    "                          )\n",
    "            newfile.close()\n",
    "            if files[0:18] not in Files_list:\n",
    "                Files_list.append(files[0:18])\n",
    "\n",
    "    #Variables used to recall indexed values.\n",
    "    var1 = 0\n",
    "\n",
    "    #Step 2: Look through the folder and locate the annotation file(s).\n",
    "    # These can be in either .txt or .ann file types.\n",
    "    for files in os.listdir(folder):\n",
    "        if Files_list[var1] and files[-4:] == \".txt\" or files[-4:] == \".ann\":\n",
    "            #Step 3: Once located, find the info we are interested in and append it to\n",
    "            # the appropriate list. We limit the variables to <=1 so that they only\n",
    "            # return two values (one for each polarization of\n",
    "            searchfile = codecs.open(files, encoding = 'windows-1252', errors='ignore')\n",
    "            for line in searchfile:\n",
    "                if \"Ground Range Data Latitude Lines\" in line:\n",
    "                    Lines = line[65:70]\n",
    "                    if verbose:\n",
    "                        print(f\"Number of Lines: {Lines}\")\n",
    "                    if Lines not in Lines_list:\n",
    "                        Lines_list.append(Lines)\n",
    "\n",
    "                elif \"Ground Range Data Longitude Samples\" in line:\n",
    "                    Samples = line[65:70]\n",
    "                    if verbose:\n",
    "                        print(f\"Number of Samples: {Samples}\")\n",
    "                    if Samples not in Samples_list:\n",
    "                        Samples_list.append(Samples)\n",
    "\n",
    "                elif \"Ground Range Data Starting Latitude\" in line:\n",
    "                    Latitude = line[65:85]\n",
    "                    if verbose:\n",
    "                        print(f\"Top left lat: {Latitude}\")\n",
    "                    if Latitude not in Latitude_list:\n",
    "                        Latitude_list.append(Latitude)\n",
    "\n",
    "                elif \"Ground Range Data Starting Longitude\" in line:\n",
    "                    Longitude = line[65:85]\n",
    "                    if verbose:\n",
    "                        print(f\"Top left Lon: {Longitude}\")\n",
    "                    if Longitude not in Longitude_list:\n",
    "                        Longitude_list.append(Longitude)\n",
    "    \n",
    "                        \n",
    "                 \n",
    "            #Reset the variables to zero for each different flight date.\n",
    "            var1 = 0\n",
    "            searchfile.close()\n",
    "\n",
    "\n",
    "    # Step 3: Open .hdr file and replace data for all type 4 (real numbers) data\n",
    "    # this all the .grd files expect for .int\n",
    "    for files in os.listdir(folder):\n",
    "        if files[-4:] == \".hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = DATTYPE\" in line:\n",
    "                        sources.write(re.sub(line[12:19], \"4\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "    \n",
    "    # Step 3: Open .hdr file and replace data for .int file date type 6 (complex)                 \n",
    "    for files in os.listdir(folder):\n",
    "        if files[-8:] == \".int.hdr\":\n",
    "            with open(files, \"r\") as sources:\n",
    "                lines = sources.readlines()\n",
    "            with open(files, \"w\") as sources:\n",
    "                for line in lines:\n",
    "                    if \"data type = 4\" in line:\n",
    "                        sources.write(re.sub(line[12:13], \"6\", line))\n",
    "                    elif \"DESCFIELD\" in line:\n",
    "                        sources.write(re.sub(line[15:24], folder, line))\n",
    "                    elif \"lines\" in line:\n",
    "                        sources.write(re.sub(line[8:13], Lines, line))\n",
    "                    elif \"samples\" in line:\n",
    "                        sources.write(re.sub(line[10:15], Samples, line))\n",
    "                    elif \"LAT\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Latitude, line))\n",
    "                    elif \"LON\" in line:\n",
    "                        sources.write(re.sub(line[12:15], Longitude, line))\n",
    "                    else:\n",
    "                        sources.write(re.sub(line, line, line))\n",
    "                        \n",
    "    \n",
    "    # Step 4: Now we have an .hdr file, the data is geocoded and can be loaded into python with rasterio\n",
    "    # once loaded in we use gdal.Translate to convert and save as a .tiff\n",
    "    \n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.grd')) # list all .grd files\n",
    "    for data_path in data_to_process: # loop to open and translate .grd to .tiff, and save .tiffs using gdal\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tif'), raster_dataset, format = 'COG', outputType = gdal.GDT_Float32)\n",
    "    \n",
    "    # Step 5: Save the .int raster, needs separate save because of the complex format\n",
    "    data_to_process = glob.glob(os.path.join(folder, '*.int.grd')) # list all .int.grd files (only 1)\n",
    "    for data_path in data_to_process:\n",
    "        raster_dataset = gdal.Open(data_path, gdal.GA_ReadOnly)\n",
    "        raster = gdal.Translate(os.path.join(folder, os.path.basename(data_path) + '.tif'), raster_dataset, format = 'COG', outputType = gdal.GDT_CFloat32)\n",
    "        \n",
    "    return\n",
    "\n",
    "def s3_bucket_transfer(folder):\n",
    "    \"\"\"\n",
    "    transfers converted .tiff files to the a3 cloud\n",
    "    :param folder:  (filepath) to folder containing the UAVSAR .tiff and .ann files\n",
    "    \"\"\"\n",
    "    num_tiffs = len(glob.glob(folder + \"*.tiff\"))\n",
    "    \n",
    "    for tiff in glob.glob(folder+ \"*.tiff\"):\n",
    "        base_name = tiff.split('/')[-1]\n",
    "        \n",
    "        region = base_name.split('_')[0]\n",
    "        year = '20' + base_name.split('_')[2][0:2]\n",
    "        flight_num = base_name.split('_')[2][2:5]\n",
    "        flight_heading = base_name.split('_')[1][0:3]\n",
    "        days_between = base_name.split('_')[4]\n",
    "        folder_name = '{}_{}_{}_{}_{}/'.format(region,year,flight_num, flight_heading, days_between)\n",
    "        \n",
    "        tiff_folder_fp = os.path.join(folder, folder_name)\n",
    "        if not os.path.exists(tiff_folder_fp):\n",
    "            os.mkdir(tiff_folder_fp)\n",
    "        \n",
    "\n",
    "        os.replace(tiff, tiff_folder_fp + base_name )\n",
    "    cmd = f'aws s3 cp {tiff_folder_fp} s3://snowex-data/uavsar-project/{folder_name} --recursive'\n",
    "    os.system(cmd)\n",
    "    \n",
    "    # Check if every file was safely uploaded\n",
    "    cmd = f'aws s3 ls s3://snowex-data/uavsar-project/{folder_name} | wc -l'\n",
    "    stream = os.popen(cmd)\n",
    "    output = stream.read()\n",
    "    if int(output) == num_tiffs:\n",
    "        print('Upload to s3 complete')\n",
    "    else:\n",
    "        print('Upload to s3 went wrong, some files are missing.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-distance",
   "metadata": {},
   "source": [
    "### Testing playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "social-analyst",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gran Mesa polygon for testing purposes\n",
    "gran_mesa_polygon = Polygon([(-108.2883, 38.882), (-108.0001, 38.882), (-108.0001, 39.0256), (-108.2883, 39.0256), (-108.2883, 38.882)])\n",
    "start_date = datetime.strptime('2017-01-10 11:00:00', '%Y-%m-%d %H:%M:%S') \n",
    "end_date = datetime.strptime('2017-02-01 11:00:00', '%Y-%m-%d %H:%M:%S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bored-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "interim-contents",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = search_asf(platform='UAVSAR', processingLevel='INTERFEROMETRY_GRD', start=start_date, end=end_date, polygon=gran_mesa_polygon, output_format='JSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "offshore-chance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'absoluteOrbit': '16095',\n",
       " 'beamMode': 'RPI',\n",
       " 'beamModeType': 'RPI',\n",
       " 'beamSwath': None,\n",
       " 'browse': 'https://datapool.asf.alaska.edu/BROWSE/UA/GrMesa_08112_16095-004_17091-004_0313d_s01_L090HH_01.cor.png',\n",
       " 'catSceneId': None,\n",
       " 'centerLat': '39.1204',\n",
       " 'centerLon': '-107.934',\n",
       " 'collectionName': 'Grand Mesa, CO',\n",
       " 'configurationName': 'Repeat Pass Interferometry',\n",
       " 'doppler': '-1',\n",
       " 'downloadUrl': 'https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01_int_grd.zip',\n",
       " 'farEndLat': '39.2679',\n",
       " 'farEndLon': '-107.5353',\n",
       " 'farStartLat': '39.0874',\n",
       " 'farStartLon': '-107.4913',\n",
       " 'faradayRotation': None,\n",
       " 'fileName': 'GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01_int_grd.zip',\n",
       " 'finalFrame': '782',\n",
       " 'firstFrame': '782',\n",
       " 'flightDirection': None,\n",
       " 'flightLine': '08112',\n",
       " 'formatName': None,\n",
       " 'frameNumber': '782',\n",
       " 'frequency': None,\n",
       " 'granuleName': 'UA_GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01',\n",
       " 'granuleType': 'UAVSAR_INSAR_SCENE',\n",
       " 'groupID': 'UA_GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01',\n",
       " 'incidenceAngle': None,\n",
       " 'insarGrouping': '0',\n",
       " 'insarStackSize': '0',\n",
       " 'lookDirection': 'L',\n",
       " 'masterGranule': None,\n",
       " 'md5sum': 'a54d27c5e07163d6a96444af24672a04',\n",
       " 'missionName': 'Grand Mesa, CO',\n",
       " 'nearEndLat': '39.1583',\n",
       " 'nearEndLon': '-108.3709',\n",
       " 'nearStartLat': '38.9673',\n",
       " 'nearStartLon': '-108.3211',\n",
       " 'offNadirAngle': '-1',\n",
       " 'percentCoherence': None,\n",
       " 'percentTroposphere': None,\n",
       " 'percentUnwrapped': None,\n",
       " 'platform': 'UAVSAR',\n",
       " 'pointingAngle': None,\n",
       " 'polarization': 'HH',\n",
       " 'processingDate': '2018-10-02T18:05:14Z',\n",
       " 'processingDescription': 'Ground projected interferogram product, one file per pair of repeat tracks, complex floating point format 8 bytes per pixel, little endian.',\n",
       " 'processingLevel': 'INTERFEROMETRY_GRD',\n",
       " 'processingType': 'L1',\n",
       " 'processingTypeDisplay': 'Ground Projected Interferogram',\n",
       " 'productName': 'UA_GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01',\n",
       " 'product_file_id': 'UA_GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01-INTERFEROMETRY_GRD',\n",
       " 'relativeOrbit': '08112',\n",
       " 'sarSceneId': None,\n",
       " 'sceneDate': '2017-09-05T18:10:57Z',\n",
       " 'sceneDateString': None,\n",
       " 'sceneId': 'UA_GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01',\n",
       " 'sensor': 'UAVSAR',\n",
       " 'sizeMB': '631.13',\n",
       " 'slaveGranule': None,\n",
       " 'startTime': '2016-10-27T22:43:32Z',\n",
       " 'status': None,\n",
       " 'stopTime': '2017-09-05T18:10:57Z',\n",
       " 'stringFootprint': 'POLYGON((-108.370894 39.158273,-107.535324 39.267869,-107.491299 39.087389,-108.321074 38.967332,-108.370894 39.158273))',\n",
       " 'thumbnailUrl': 'https://datapool.asf.alaska.edu/THUMBNAIL/UA/GrMesa_08112_16095-004_17091-004_0313d_s01_L090HH_01.unw_THUMBNAIL.jpg',\n",
       " 'track': '08112',\n",
       " 'varianceTroposphere': None}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cubic-might",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 product(s) found\n",
      "Downloaded 99% with 339M per second and 0s left....\n",
      "Done downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/GrMesa_26108_16095-005_17091-005_0313d_s01_L090_01_int_grd.zip\n",
      "Unzipping product\n",
      "Deleting original .zip\n",
      "Downloaded 99% with 7.11M per second and 0s left...\n",
      "Done downloading https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/GrMesa_08112_16095-004_17091-004_0313d_s01_L090_01_int_grd.zip\n",
      "Unzipping product\n",
      "Deleting original .zip\n"
     ]
    }
   ],
   "source": [
    "search_and_bulk_download(platform='UAVSAR', processingLevel='INTERFEROMETRY_GRD', start=start_date, end=end_date, polygon=gran_mesa_polygon, path='/tmp/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strange-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".tiffs have been created\n",
      "Uploading to S3 bucket\n",
      "Upload to s3 complete\n",
      ".tiffs have been created\n",
      "Uploading to S3 bucket\n",
      "Upload to s3 complete\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir('/tmp/'):\n",
    "    if not folder.startswith('.'):\n",
    "        print('Convertig images to Cloud Optimized .tiff')\n",
    "        uavsar_tiff_convert(f'/tmp/{folder}/')\n",
    "        print('Uploading to S3 bucket')\n",
    "        s3_bucket_transfer(f'/tmp/{folder}/')\n",
    "        os.system(f'rm -rf /tmp/{folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-imaging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
