{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e99033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "## feb 11, 2022\n",
    "## testing the functions zach wrote for data download\n",
    "## doesn't use wget and fully \"pythonized\" - uses requests instead\n",
    "## using asf api to generate URLS\n",
    "\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import zipfile\n",
    "import getpass\n",
    "import os  # for chdir, getcwd, path.basename, path.exists\n",
    "import pandas as pd # for DatetimeIndex\n",
    "import netrc\n",
    "import glob\n",
    "import requests\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from datetime import datetime\n",
    "from subprocess import PIPE, Popen\n",
    "from os.path import join, isdir, isfile, basename\n",
    "import progressbar\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "logging.basicConfig()\n",
    "log.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44601ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Username:  jacktarricone\n",
      "Enter Password:  ············\n"
     ]
    }
   ],
   "source": [
    "# input NASA Earthdata credentials here\n",
    "ASF_USER = input(\"Enter Username: \")\n",
    "ASF_PASS = getpass.getpass(\"Enter Password: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c66bfae",
   "metadata": {},
   "source": [
    "### Function definitions to query ASF APi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c4a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bbox_string(polygon):\n",
    "    '''\n",
    "    Builds the string to include in the ASF search request. The bbox consists of 4 comma-separated numbers: lower left longitude,latitude, and upper right longitude,latitude.\n",
    "    \n",
    "    Parmeters\n",
    "    ----------\n",
    "    polygon: shapely polygon\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    polygon_string : string\n",
    "        String to include in the ASF request\n",
    "    '''\n",
    "    points = mapping(polygon)['coordinates'][0]\n",
    "    lower_left = points[0]\n",
    "    upper_right = points[2]\n",
    "    bbox_string = f'{lower_left[0]},{lower_left[1]},{upper_right[0]},{upper_right[1]}'\n",
    "        \n",
    "    return bbox_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3652bf-c7b9-4abf-a1ac-2905e0d6960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_asf(platform, processingLevel, start, end, polygon, output_format):\n",
    "    '''\n",
    "    Search the ASF platform for images given the input parameters\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        platform : string\n",
    "            Name of the imaging platform. Defaults to UAVSAR, but a list of supported platform is available on the ASF website\n",
    "        processingLevel : string\n",
    "            Processing level of the imaging product. \n",
    "            Possible values for UAVSAR : (KMZ, PROJECTED, PAULI, PROJECTED_ML5X5, STOKES, AMPLITUDE, BROWSE, COMPLEX, DEM_TIFF, PROJECTED_ML3X3, METADATA, AMPLITUDE_GRD, INTERFEROMETRY, INTERFEROMETRY_GRD, THUMBNAIL)\n",
    "        start : datetime object\n",
    "            Start date of the search period.\n",
    "        end : datetime object\n",
    "            End date of the search period.\n",
    "        polygon : shapely polygon defining the Area of Interest,\n",
    "        output_format: string\n",
    "            Format being returned by the ASF API. Values : CSV, JSON, KML, METALINK, COUNT, DOWNLOAD, GEOJSON\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Ouputs a search file\n",
    "    '''\n",
    "    base = 'https://api.daac.asf.alaska.edu/services/search/param'\n",
    "    start_date = start.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "    end_date = end.strftime('%Y-%m-%dT%H:%M:%SUTC')\n",
    "    aoi_string = build_bbox_string(polygon)\n",
    "    payload = {\n",
    "        'platform': platform,\n",
    "        'processingLevel': processingLevel,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'bbox': aoi_string,\n",
    "        'output': output_format\n",
    "    }\n",
    "    r = requests.get(base, params=payload)\n",
    "    \n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de3fa148-1528-4df8-8c21-49af62481134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_download(url, output_f):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        url: url to download\n",
    "        output_f: path to save the data to\n",
    "    \"\"\"\n",
    "\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        # Progress bar - https://towardsdatascience.com/how-to-download-files-using-python-part-2-19b95be4cdb5\n",
    "        total_size= int(r.headers.get('content-length', 0))\n",
    "        with open(output_f, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True , desc=f'Downloading {basename(url)}') as pbar:\n",
    "                for ch in r.iter_content(chunk_size=1024):\n",
    "                    if ch:\n",
    "                        f.write(ch)\n",
    "                        pbar.update(len(ch))\n",
    "    else:\n",
    "        log.warning(f'HTTP CODE {r.status_code}. Skipping download!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2096cbea-1b95-4666-a26b-b17ce07c866c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, output_dir, ann = False):\n",
    "    \"\"\"\n",
    "    Downloads uavsar InSAR files from a url.\n",
    "    Args:\n",
    "        url (string): A url containing uavsar flight data. Can be from JPL or ASF\n",
    "        output_dir (string): Directory to save the data in\n",
    "    Returns:\n",
    "        out_fp (string): File path to downloaded image.\n",
    "    Raises:\n",
    "       None\n",
    "    \"\"\"\n",
    "\n",
    "    log.info(f'Starting download of {url}...')\n",
    "    local = join(output_dir, basename(url))\n",
    "\n",
    "    # Make the output dir if it doesn't exist\n",
    "    if not isdir(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    if not isfile(local):\n",
    "        stream_download(url, local)\n",
    "    else:\n",
    "        log.info(f'{local} already exists, skipping download!')\n",
    "\n",
    "    if ann:\n",
    "        if url.split('.')[-1] == 'zip' or url.split('.')[-1] == 'ann':\n",
    "            log.info('Download already contains ann file, skipping download!')\n",
    "        else:\n",
    "            parent = dirname(url)\n",
    "            # ASF formatting - query parent directory\n",
    "            if parent.split('.')[-1] == 'zip':\n",
    "                log.debug(f'ASF url found for {url}')\n",
    "                parent_files = requests.get(parent).json()['response']\n",
    "                ann_info = [i for i in parent_files if '.ann' in i['name']][0]\n",
    "                # assert len(ann_info) == 1, 'More than one ann file detected'\n",
    "                ann_url = ann_info['url']\n",
    "                log.debug(f'Annotation url: {ann_url}')\n",
    "\n",
    "            # JPL formatting - have to parse url to get ann\n",
    "            elif 'uavsar.asfdaac.alaska.edu' in url:\n",
    "                log.debug(f'JPL url found for {url}')\n",
    "                ext = url.split('.')[-1]\n",
    "                pols = ['VVVV','HHHH','HVHV', 'HHHV', 'HHVV','HVVV']\n",
    "                slc_pol = [pol for pol in pols if (pol in url)]\n",
    "                if len(slc_pol) == 1:\n",
    "                    url = url.replace(slc_pol[0], '')\n",
    "\n",
    "                if ext == 'grd':\n",
    "                    if len(basename(url).split('.')) == 2:\n",
    "                        url = url.replace('.grd','.ann')\n",
    "                    if len(basename(url).split('.')) == 3:\n",
    "                        url = url.replace('.grd','')\n",
    "                    ext = url.split('.')[-1]\n",
    "                ann_url = url.replace(f'.{ext}', '.ann')\n",
    "                log.debug(f'Annotation url: {ann_url}')\n",
    "\n",
    "            else:\n",
    "                log.warning('No ann url found. Unable to download ann file.')\n",
    "                ann_url = None\n",
    "\n",
    "            if ann_url:\n",
    "                ann_local = join(output_dir, basename(ann_url))\n",
    "                log.debug(f'Annotation local: {ann_local} and url {ann_url}')\n",
    "                if not isfile(ann_local):\n",
    "                    stream_download(ann_url, ann_local)\n",
    "                else:\n",
    "                    log.info(f'{ann_local} already exists, skipping download!')\n",
    "    return local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4459e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search parameters for sierra flight line\n",
    "sierra_polygon = Polygon([(-119.9697,37.4631),(-118.9576,37.4631),(-118.9576,38.7211),(-119.969,38211)])\n",
    "start_date = datetime.strptime('2020-02-28 11:00:00', '%Y-%m-%d %H:%M:%S') \n",
    "end_date = datetime.strptime('2020-03-11 11:00:00', '%Y-%m-%d %H:%M:%S') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd735df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 product(s) found\n"
     ]
    }
   ],
   "source": [
    "# query API to generate one over pass\n",
    "results = search_asf(platform='UAVSAR', processingLevel='INTERFEROMETRY_GRD', \n",
    "                    start=start_date, end=end_date, polygon=sierra_polygon, output_format='JSON')[0]\n",
    "\n",
    "# print number of products\n",
    "print(f'{len(results)} product(s) found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebf63dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://datapool.asf.alaska.edu/INTERFEROMETRY_GRD/UA/sierra_17305_20014-000_20016-005_0014d_s01_L090_01_int_grd.zip\n"
     ]
    }
   ],
   "source": [
    "# define url\n",
    "for i in results:\n",
    "        downloadUrl = i['downloadUrl']\n",
    "        print(downloadUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35d7d1-ebdb-41d6-a725-3bb855ef63cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading sierra_17305_20014-000_20016-005_0014d_s01_L090_01_int_grd.zip:   7%"
     ]
    }
   ],
   "source": [
    "download_image(url = downloadUrl, \n",
    "               output_dir = '/Users/jacktarricone/Desktop/zach_test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f772d3-f4e3-4d30-8971-0326a35d13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloadUrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e37fbd0-5c1e-4129-86e4-7767b6d48c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde0136-ca27-4656-a0f5-2c0f8351fb71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d1f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get new path for folder of insar data just downloaded\n",
    "new_path_list = glob.glob('/Users/jacktarricone/ch2_sierra_data/sierra/*')\n",
    "new_path = new_path_list[3] # select first list elemet\n",
    "print(new_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd172d8c-6a1d-4e42-9dd8-b4d92325f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5cfadb-9bf9-43e3-a007-aa3e7ff67b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddc872-16d4-4c8a-a875-91e8c62ddebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19265e1-f59e-4deb-81c0-9956226bae49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test_env]",
   "language": "python",
   "name": "conda-env-test_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
